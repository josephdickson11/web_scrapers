{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<img src=\"./gifs/gif2.gif\", width=\"800\", height=\"200\", class=\"center\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebooks agenda!!\n",
    "\n",
    "## This particular notebook will focus on introducing two powerful tools in tiny little nuggets\n",
    "\n",
    "#### Number 1: introduce the request and BeautifulSoup modules of python\n",
    "\n",
    "#### Number 2 and my favourite nugget bank for today, we will put these two sweet modules to use by \n",
    "\n",
    "#### actuall creating a web scrapper!!\n",
    "\n",
    "\n",
    "#### Before we actually get down and dirty with these guys, i would strongly recommend you only use this notebook as a quick \n",
    "\n",
    "#### starter to whats possible in the universe of webscrapping\n",
    "\n",
    "#### kindly refer to the readMe file for recommended materials to scrape like a web crawler!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"./gifs/gif3.gif\", width=\"800\", height=\"200\", class=\"center\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meet the two modules we will be using today by IMPORTING them to the party\n",
    "\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: \n",
    "      request module will definitely come prepackaged with your base environment in anaconda, \n",
    "      BeautifulSoup might not be so friendly so just go ahead and run this command \"pip install beautifulsoup4\" in your command       prompt and you are good to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Scraping process with request and beautifulsoup\n",
    "\n",
    "\n",
    "#### 1. You first want to find the webpage that contains the data you want to scrape\n",
    "\n",
    "#### 2. Next thing to do would be to study the structure of the said page, developers tools would be very useful to do this. Easily right \n",
    "\n",
    "#### click anywhere on the webpage and select the Inspect option for a quick start.\n",
    "\n",
    "#### 3. The next thing you want to do is to locate the data you want to scrape and the HTML element tree that takes you there\n",
    "\n",
    "#### 4. Now, you write the code to collect the data and store in your desired format!!\n",
    "\n",
    "#### Now, lets do some rough introduction to the stars of our party before we get down to the business end.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The webpage we will be practising with is the wikipedia page of the legendary Black Panther!! May His Soul Rest in Power\n",
    "\n",
    "https://en.wikipedia.org/wiki/Chadwick_Boseman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we get this webpage and assign it to the variable URL(choosen strictly by convension)\n",
    "\n",
    "URL = \"https://en.wikipedia.org/wiki/Chadwick_Boseman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we have the request module get this page for us and localize it on our local machine so we can work with it.\n",
    "r = req.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we pass the webpage to beautifulsoup as a variable, along with a an html parser to parse the html page we are getting from request\n",
    "soup = BeautifulSoup(r.content, 'lxml' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets print out the soup and see what we have\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "    The ouput of our soup actually doesn't look that beautiful as you can see. it's actually difficult to understand what is       going in the output. check the next code line to see how we can resolve this...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "    by calling the prettify() method on the soup, we have been able to add the necessary ingredients to make our soup    beautiful, much readable and obviously understandable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can access page data by going through the  elements tree and calling the containing element as demonstrated in this  code cell\n",
    "title = soup.h1.text\n",
    "#print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the same manner, we access the table element aand try to get all the tables present in the page through the find_all() method\n",
    "\n",
    "tables = soup.find_all(\"table\")\n",
    "#print(len(tables))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "    A list is created of all the tables and individual lists can be accessed by calling their indexes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling the table stored in index 1\n",
    "tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#like wise a list of all \"li\" elements can also be created and its children can the be assessed through the \".children\" method\n",
    "lists = soup.find_all(\"li\")\n",
    "#print(len(lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting children of lists\n",
    "childs = list(lists[3].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(childs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#searching for elements by class and attributes can aslo be done. \n",
    "#here we retrieve all the links on the page by accessing it \"a\" tag\n",
    "\n",
    "#links = soup.find_all(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returning the first five elements of the link list\n",
    "#links[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#likewise attribute filters can also be created to help access multi layered elements at a go as demonstrated below\n",
    "# the attribute filters are then passed as parameters to the find or findall methods\n",
    "attr_filter = {\"class\":\"mw-jump-link\"}\n",
    "soup.find_all(\"a\", attr_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more on attribute filters\n",
    "#the code below demonstrates that when attribute filters are passed with no binding html elements, the find/find_all method\n",
    "#retrieves all elements with the attributes regardless of the binding html element\n",
    "\n",
    "attr_filter = {\"class\":\"noprint\"}\n",
    "soup.find_all(None, attr_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more on filters\n",
    "#filter by css\n",
    "\n",
    "attr_filter = {\"class\":\"fn\"}\n",
    "#soup.find_all(None, attr_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding an element by Id\n",
    "\n",
    "attr_filter = {\"id\":\"firstHeading\"}\n",
    "#soup.find_all(None, attr_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tables[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_attr = {\"scope\":\"col\"}\n",
    "childs_table = list(tables[4].children)\n",
    "#childs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection can also be done through html selectors\n",
    "selector = \"#mw-content-text > div.mw-parser-output > div.thumb.tright > div > a > img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.select(selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the basics together to scrape data from a website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. You first want to find the webpage that contains the data you want to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#webpage to be scraped is the IMDB movie filters result page\n",
    "\n",
    "URL = 'https://www.imdb.com/search/title/?release_date=2018-01-01,2020-01-01'\n",
    "r = req.get(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Next thing to do would be to study the structure of the said page, developers tools would be very useful to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"./gifs/Screenshot (44).png\", width=\"800\", height=\"200\", class=\"center\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The next thing you want to do is to locate the data you want to scrape and the HTML element tree that takes you there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cases = soup.find_all('div', class_ = \"lister-item mode-advanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(movie_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "    The data we need are all children of the movie_cases, now we can go on and code !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the scraped data in\n",
    "names = []\n",
    "years = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "# Extract data from individual movie container\n",
    "for container in movie_cases:\n",
    "# If the movie has Metascore, then extract:\n",
    "    if container.find('div', class_ = 'ratings-metascore') is not None:\n",
    "# The name\n",
    "        name = container.h3.a.text\n",
    "        names.append(name)\n",
    "# The year\n",
    "        year = container.h3.find('span', class_ = 'lister-item-year').text\n",
    "        years.append(year)\n",
    "# The IMDB rating\n",
    "        imdb = float(container.strong.text)\n",
    "        imdb_ratings.append(imdb)\n",
    "# The Metascore\n",
    "        m_score = container.find('span', class_ = 'metascore').text\n",
    "        metascores.append(int(m_score))\n",
    "# The number of votes\n",
    "        vote = container.find('span', attrs = {'name':'nv'})['data-value']\n",
    "        votes.append(int(vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store collected data in a pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "movies_df = pd.DataFrame({'movie_title':names, 'release_year':years, 'rating':imdb_ratings, 'votes':votes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the head of the newly scraped dataset\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"./gifs/wakanda.gif\", width=\"800\", height=\"200\", class=\"center\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are done, I hope you learnt a thing or two, please feel free to reach out if \n",
    "\n",
    "####  you have a correction, an improvement or a request, i will be glad to help out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
